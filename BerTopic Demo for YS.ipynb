{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJDjm9f2kGFIBdRkUIO0Jr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# BertTopic Demo \n","From the result, the model we found for suicidal text that was most optimal was the BerTopic model"],"metadata":{"id":"C3gUIH3OTunm"}},{"cell_type":"markdown","source":["Download stuff (if u dun have them)"],"metadata":{"id":"xHCNJ6OgUIn0"}},{"cell_type":"code","source":["!pip install bertopic\n","!pip install symspellpy\n","!pip install pandas==1.3.5\n","!pip install joblib==1.2.0\n"],"metadata":{"id":"4KO94RDbZRJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install numpy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejkRHNMRrvKg","executionInfo":{"status":"ok","timestamp":1679125582898,"user_tz":-480,"elapsed":6618,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"a4d6674f-ebda-4238-a1a8-5fbfc943caee"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n"]}]},{"cell_type":"markdown","source":["Load libs & files"],"metadata":{"id":"EIcdboccZpK5"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBQtRmKOToxn","executionInfo":{"status":"ok","timestamp":1679122044508,"user_tz":-480,"elapsed":23673,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"b9152cfc-a7d5-40fc-e9a7-8e5779e24053"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/TxtAnalytics_Proj\n"]}],"source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","import bertopic\n","import pandas as pd\n","from bertopic import BERTopic\n","import joblib\n","\n","#read from gdrive\n","import pandas as pd\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","%cd gdrive/MyDrive/TxtAnalytics_Proj\n","\n"]},{"cell_type":"code","source":["\n","#print('The pickle version is {}.'.format(pickle.__version__))\n","print('The bertopic version is {}.'.format(np.__version__))\n","topic_model = BERTopic.load('bertTopic_suicidal_model.jl')\n","Probs = joblib.load('bertTopic_suicidal_probs.jl')\n","topics = joblib.load('bertTopic_suicidal_topic.jl')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"JbDvjkdBcVHM","executionInfo":{"status":"error","timestamp":1679125540086,"user_tz":-480,"elapsed":842,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"e835e4e8-0032-4d65-8e4e-252e0462fa11"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["The bertopic version is 1.22.4.\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-bf6104002f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The bertopic version is {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#topic_model = BERTopic.load('bertTopic_suicidal_model.jl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mProbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bertTopic_suicidal_probs.jl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bertTopic_suicidal_topic.jl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# More user-friendly error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         new_exc = ValueError(\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0;34m'You may be trying to read with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m             \u001b[0;34m'python 3 a joblib pickle generated with python 2. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             'This feature is not supported by joblib.')\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# Raising an error if a non valid compress level is given.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         raise ValueError(\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0;34m'Non valid compress level given: \"{}\". Possible values are '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m             '{}.'.format(compress_level, list(range(10))))\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 231"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"1xvCCiEPWCHa"}},{"cell_type":"code","source":["from nltk import pos_tag, word_tokenize\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import gensim\n","import symspellpy\n","from symspellpy import SymSpell, Verbosity\n","import pkg_resources\n","\n","def get_wordnet_pos(treebank_tag):\n","  \"\"\"\n","  return WORDNET POS compliance to WORDNET lemmatization (a,n,r,v) \n","  \"\"\"\n","  if treebank_tag.startswith('J'):\n","    return wordnet.ADJ\n","  elif treebank_tag.startswith('V'):\n","    return wordnet.VERB\n","  elif treebank_tag.startswith('N'):\n","    return wordnet.NOUN\n","  elif treebank_tag.startswith('R'):\n","    return wordnet.ADV\n","  else:\n","  # As default pos in lemmatization is Noun\n","    return wordnet.NOUN\n","\n","def pos_tag_and_lemmatize(sentence):\n","# find the pos tagging for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n","\n","  pos_tokens = [nltk.pos_tag(word_tokenize(sentence))]\n","  lemmatizer = WordNetLemmatizer()\n","  # lemmatization using pos tag  \n","  pos_tokens = [ [(token, lemmatizer.lemmatize(token, get_wordnet_pos(pos_tag)), [pos_tag]) for (token,pos_tag) in pos] for pos in pos_tokens]\n","  return pos_tokens\n","\n","def getLemmatizedText(pos_tag):\n","  lst = list()\n","  for postag_tuple in pos_tag:\n","    for p in postag_tuple:\n","      value = re.sub(r'[^a-zA-Z ]', '', p[1])\n","      lst.append(value)\n","\n","  return lst\n","\n","def preprocess(text):\n","  #to lower case\n","  result = text.str.lower()\n","\n","  #remove punctuation and numbers\n","  result = result.apply(lambda t: re.sub(r'[^a-zA-Z ]', '', t))\n","\n","  result = result.apply(lambda t: gensim.parsing.preprocessing.remove_stopwords(str(t)))\n","\n","  symsp = SymSpell(max_dictionary_edit_distance=1, prefix_length=7) #Damerau-Levenshtein algorithm = 2, Levenshtein = 1\n","  dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","  symsp.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","  result = result.apply(lambda t: symsp.lookup_compound(t, max_edit_distance=1)[0].term )\n","  result = result.apply(lambda t: gensim.parsing.preprocessing.remove_stopwords(str(t)))\n","\n","   #lemmatization, note: Lemmatization includes stemming as discussed in class.\n","  result_pos_tag = result.apply(lambda t: pos_tag_and_lemmatize(t))\n","\n","  result_lemmatized = result_pos_tag.apply(lambda t: getLemmatizedText(t))\n","\n","  return result_lemmatized\n"],"metadata":{"id":"rqfPv__QWCXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_topic(model, vectorizer, text):\n","\n","    if len(text) == 0:\n","      return None, -1\n","    else:\n","      processedText = preprocess(text)\n","     \n","      topics, topic_probability_scores = model.transform(processedText)\n","      top_topics = np.argmax(topic_probability_scores, axis=1)\n","\n","      # Map topic IDs to topic labels\n","      topic_labels = model.get_topic_freq().Topic.to_dict()\n","\n","      # Get the topic labels for each document\n","      topic_labels = [topic_labels.get(topic_id) for topic_id in top_topics]\n","\n","      # Print the topic labels for each document\n","      infer_topicNo = topic_labels[0]\n","      topic_words = model.get_topic(topic=infer_topicNo)\n","      word_lists = []\n","      for w in topic_words:\n","          word_lists.append(w[0])\n","\n","      topic = word_lists\n","      #print (topic)\n","      #for i, text in enumerate(text):\n","      #    print(f\"Document {i}: nTopic: {topic_labels[i]}\")\n","      return topic, infer_topicNo"],"metadata":{"id":"FqXpBxNrV_2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Usage"],"metadata":{"id":"PB1Vr0wrac9T"}},{"cell_type":"code","source":["mytext = [\"\"\" i wanna jump from an elevated place. i wanna kill myself, like really really would like to jump off an elevated place, but not die what do i do? drugs? fall from somewhere not far above the ground(like a 4meters jump)? you know, i'd just like to die but i have a gr8 life other than that (I have different lives in two different countries, just want to kill myself in one)\n","\n","         \"\"\"]\n","\n","topic, infer_topicNo = predict_topic(topic_model, vectorizer = topic_model.vectorizer_model, text = mytext)\n","\n","print(\"Topic: \" + str(infer_topicNo) )\n","print(topic)"],"metadata":{"id":"OIxTrb0Rabr_"},"execution_count":null,"outputs":[]}]}