{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN5TtVcbY0w9i/2ClTXj4qn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf7572a3","executionInfo":{"status":"ok","timestamp":1678253456215,"user_tz":-480,"elapsed":51971,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"43f0fa12-b085-4245-fefc-7355dcab4140"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/TxtAnalytics_Proj\n"]}],"source":["from nltk.classify.scikitlearn import SklearnClassifier\n","from sklearn.decomposition import NMF\n","\n","import re # The following statement imports the regex package.\n","import gensim # The following statement imports the gensim package.\n","import nltk # The following statement imports the NLTK package.\n","import pkg_resources\n","import pandas as pd\n","import os\n","import numpy as np\n","#from symspellpy import SymSpell, Verbosity\n","from nltk import pos_tag, word_tokenize\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","%cd gdrive/MyDrive/TxtAnalytics_Proj"]},{"cell_type":"code","source":["def generateVectors(posts, filename):\n","         #create dictionary\n","        all_docs5 = []\n","\n","        suicidal_data_TFIDF = list()\n","        suicidal_data_TF = list()\n","\n","        #create corpus dictionary\n","        corpus_dictionary = gensim.corpora.Dictionary()\n","\n","        stop_list = (\"suicidal\", \"suicide\", \"aah\", \"fuck\")\n","        \n","        \n","        for i in range(len(posts)):\n","            value = posts.iloc[i, posts.columns.get_loc(\"text_lemmatized\")]\n","            #print(value)\n","            all_docs5.append(value) #df.iloc[0, df.columns.get_loc(\"a\")]\n","\n","        all_docs5 = [[w for w in doc if w not in stop_list] for doc in all_docs5]\n","               \n","        corpus_dictionary.add_documents(all_docs5)\n","        \n","        # Convert all documents to term frequency (TF) vectors\n","        all_tf_vectors = [corpus_dictionary.doc2bow(doc) for doc in all_docs5]\n","\n","        #ntc = n = raw, t = zero-corrected idf, c = cosine - https://radimrehurek.com/gensim/models/tfidfmodel.html\n","        tfidf = gensim.models.TfidfModel(all_tf_vectors, smartirs='ntc')\n","        corpus_tfidf = tfidf[all_tf_vectors]\n","        \n","        all_data_as_dict = [{id:tf_value for (id, tf_value) in vec} for vec in all_tf_vectors]\n","        tfidf_data_as_dict = [{id:tf_value for (id, tf_value) in vec} for vec in corpus_tfidf]\n","       \n","        for i in range(len(all_data_as_dict)):\n","            doc_tf = all_data_as_dict[i]\n","            doc_tfidf = tfidf_data_as_dict[i]\n","\n","            suicidal_data_TF.append((doc_tf, 1))\n","            suicidal_data_TFIDF.append((doc_tfidf, 1))\n","\n","      \n","        #print(suicidal_data_TFIDF)\n","        posts['tf_vector'] = suicidal_data_TF \n","        posts['tfidf_vector']  = suicidal_data_TFIDF\n","\n","        posts.to_pickle(filename)\n","\n","        corpus_dictionary.save(filename.replace(\".pkl\", \".dict\"))\n","\n","        return posts"],"metadata":{"id":"FcuWZsM-Flz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data_raw =  pd.read_pickle(\"posts_after_lemmatized.pkl\")\n","!pip install pandas==1.5.3\n","print('The pandas version is {}.'.format(pd.__version__))\n","#df_filtered =  pd.read_pickle(\"posts_suicidal_with_tfidf2.pkl\")\n","#df_filtered = generateVectors(df_filtered, \"posts_suicidal_with_tfidf2.pkl\")\n","#df_filtered.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnMheYMtFBps","executionInfo":{"status":"ok","timestamp":1678205770335,"user_tz":-480,"elapsed":9557,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"25b85c89-73b8-4c41-ad21-457275a2994a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.8/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2.8.2)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.15.0)\n","The pandas version is 1.5.3.\n"]}]},{"cell_type":"code","source":["\n","\n","!pip install fast_ml\n","\n","import fast_ml\n","from fast_ml.model_development import train_valid_test_split\n","X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df_filtered, target = 'class', \n","                                                                            train_size=0.7, valid_size=0.2, test_size=0.1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"LdXYv31NL_ug","executionInfo":{"status":"error","timestamp":1678253607926,"user_tz":-480,"elapsed":4976,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"8ffdca25-06d6-4b94-c864-f606216d3e3b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fast_ml\n","  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fast_ml\n","Successfully installed fast_ml-3.68\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a4a52d11be3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfast_ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfast_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_development\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_valid_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df_filtered, target = 'class', \n\u001b[0m\u001b[1;32m      6\u001b[0m                                                                             train_size=0.7, valid_size=0.2, test_size=0.1)\n","\u001b[0;31mNameError\u001b[0m: name 'df_filtered' is not defined"]}]},{"cell_type":"code","source":["X_train.to_pickle(\"NMF_X_Train.pkl\")\n","X_test.to_pickle(\"NMF_X_Test.pkl\")\n","X_valid.to_pickle(\"NMF_X_Valid.pkl\")"],"metadata":{"id":"gPYzcx11nqOT","executionInfo":{"status":"error","timestamp":1678216188865,"user_tz":-480,"elapsed":739,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"colab":{"base_uri":"https://localhost:8080/","height":200},"outputId":"b9e94a30-f0d0-4be2-b325-70b2d7ff0940"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-02b9b9d2ce8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NMF_X_Train.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NMF_X_Test.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NMF_X_Valid.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}]},{"cell_type":"code","source":["from sklearn.decomposition import NMF\n","from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import silhouette_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Define custom scoring function\n","def my_score(X, labels):\n","    return silhouette_score(X, labels=True)\n","\n","from sklearn.metrics import mean_squared_error, mean_squared_log_error\n"],"metadata":{"id":"WhPOplR7zz0D","executionInfo":{"status":"ok","timestamp":1678253328732,"user_tz":-480,"elapsed":1278,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install pandas==1.5.3\n","import pandas as pd\n","#print('The pandas version is {}.'.format(pd.__version__))\n","\n","vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","trainingData = pd.read_pickle(\"NMF_X_Train.pkl\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eu2CIyjtnDZX","executionInfo":{"status":"ok","timestamp":1678253827413,"user_tz":-480,"elapsed":19452,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"76adf420-2a62-46dd-94e8-a7ad0dc724dd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.8/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2.8.2)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.15.0)\n"]}]},{"cell_type":"code","source":["post_split_Train_X = np.array_split(trainingData, 5)\n","searchMatrix = post_split_Train_X[0]\n","stop_list = (\"suicidal\", \"suicide\", \"aah\", \"fuck\", \"fucking\")\n","text_data = searchMatrix['text_lemmatized'].to_numpy()\n","text_data = [[w for w in doc if w not in stop_list] for doc in text_data]\n","\n","text_data_lemmatized_joined = [' '.join(x) for x in text_data]  # joined to fit CountVectorizer\n","\n","vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","\n","# Fit the vectorizer and transform the text data\n","X = vectorizer.fit_transform(text_data_lemmatized_joined)\n","\n","param_grid = {'l1_ratio': [0, 0.2, 0.25, 0.5, 1],\n","               'alpha_H': [0, 0.01, 0.001],\n","               'alpha_W': [0, 0.01, 0.001],\n","              }\n","\n","nmf = NMF(alpha_H= 0.001, random_state=10,\n","                n_components=10\n","                ,init = 'random'\n","                )\n","x_2d = X.reshape(-1, 1)\n","grid_search = HalvingGridSearchCV(estimator=nmf, param_grid=param_grid, scoring=my_score,\n","                                  aggressive_elimination = True ,\n","                                  cv=5)\n","\n","grid_search.fit(x_2d)\n"],"metadata":{"id":"_zlAqWjB4yfN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def GridSearch_table_plot(grid_clf, param_name,\n","                          num_results=15,\n","                          negative=True,\n","                          graph=True,\n","                          display_all_params=True):\n","\n","    '''Display grid search results\n","\n","    Arguments\n","    ---------\n","\n","    grid_clf           the estimator resulting from a grid search\n","                       for example: grid_clf = GridSearchCV( ...\n","\n","    param_name         a string with the name of the parameter being tested\n","\n","    num_results        an integer indicating the number of results to display\n","                       Default: 15\n","\n","    negative           boolean: should the sign of the score be reversed?\n","                       scoring = 'neg_log_loss', for instance\n","                       Default: True\n","\n","    graph              boolean: should a graph be produced?\n","                       non-numeric parameters (True/False, None) don't graph well\n","                       Default: True\n","\n","    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?\n","                       Default: True\n","\n","    Usage\n","    -----\n","\n","    GridSearch_table_plot(grid_clf, \"min_samples_leaf\")\n","\n","                          '''\n","    from matplotlib      import pyplot as plt\n","    from IPython.display import display\n","    import pandas as pd\n","\n","    clf = grid_clf.best_estimator_\n","    clf_params = grid_clf.best_params_\n","    if negative:\n","        clf_score = -grid_clf.best_score_\n","    else:\n","        clf_score = grid_clf.best_score_\n","    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]\n","    cv_results = grid_clf.cv_results_\n","\n","    print(\"best parameters: {}\".format(clf_params))\n","    print(\"best score:      {:0.10f} (+/-{:0.10f})\".format(clf_score, clf_stdev))\n","    if display_all_params:\n","        import pprint\n","        pprint.pprint(clf.get_params())\n","\n","    # pick out the best results\n","    # =========================\n","    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n","    #print(scores_df)\n","    best_row = scores_df.iloc[0, :]\n","    if negative:\n","        best_mean = -best_row['mean_test_score']\n","    else:\n","        best_mean = best_row['mean_test_score']\n","    best_stdev = best_row['std_test_score']\n","    best_param = best_row['param_' + param_name]\n","\n","    # display the top 'num_results' results\n","    # =====================================\n","    display(pd.DataFrame(cv_results) \\\n","            .sort_values(by='rank_test_score').head(num_results))\n","\n","    # plot the results\n","    # ================\n","    scores_df = scores_df.sort_values(by='param_' + param_name)\n","\n","    if negative:\n","        means = -scores_df['mean_test_score']\n","    else:\n","        means = scores_df['mean_test_score']\n","    stds = scores_df['std_test_score']\n","    params = scores_df['param_' + param_name]\n","\n","    # plot\n","    if graph:\n","        plt.figure(figsize=(8, 8))\n","        plt.errorbar(params, means, yerr=stds)\n","\n","        plt.axhline(y=best_mean + best_stdev, color='red')\n","        plt.axhline(y=best_mean - best_stdev, color='red')\n","        plt.plot(best_param, best_mean, 'or')\n","\n","        plt.title(param_name + \" vs Score\\nBest Score {:0.10f}\".format(clf_score))\n","        plt.xlabel(param_name)\n","        plt.ylabel('Score')\n","        plt.show()\n","\n","\n","# Print best parameters\n","print(grid_search.best_params_)"],"metadata":{"id":"zpteyIQtpUiV","executionInfo":{"status":"aborted","timestamp":1678220546445,"user_tz":-480,"elapsed":6,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["searchMatrix = pd.read_pickle(\"NMF_X_Train.pkl\")\n","stop_list = (\"suicidal\", \"suicide\", \"aah\", \"fuck\", \"fucking\")\n","text_data = searchMatrix['text_lemmatized'].to_numpy()\n","text_data = [[w for w in doc if w not in stop_list] for doc in text_data]\n","\n","text_data_lemmatized_joined = [' '.join(x) for x in text_data]  # joined to fit CountVectorizer\n","\n","vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","# Fit the vectorizer and transform the text data\n","data_vectorized = vectorizer.fit_transform(text_data_lemmatized_joined)\n","\n","feature_names1 = vectorizer.get_feature_names_out()\n"],"metadata":{"id":"5cJDzFGaZn3G","executionInfo":{"status":"ok","timestamp":1678213821168,"user_tz":-480,"elapsed":22114,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["\n","NMF_model = NMF(alpha_H= 0.001, \n","                #alpha_W= 0.001, \n","                l1_ratio=0.25, \n","                n_components=10\n","                ,init = 'nndsvd'\n","                )\n","\n","nmf_output = NMF_model.fit_transform(data_vectorized)\n","print(nmf_output)  # Model attributes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AksXjlDLruK","executionInfo":{"status":"ok","timestamp":1678213855437,"user_tz":-480,"elapsed":26373,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"1b9e25a1-fc62-4b06-c447-c012c2b9913e"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.          0.          0.         ...  4.41256346 10.89402751\n","   0.        ]\n"," [ 7.96657627  2.93622238  1.19345597 ...  2.42184209  3.94265621\n","   3.56154354]\n"," [14.08958421  1.07652326  0.         ...  4.17180488  0.55579553\n","   2.84125974]\n"," ...\n"," [13.38826749  0.07772303  1.66232003 ...  3.4688774   0.33474805\n","   0.        ]\n"," [14.56718567  1.62035948  0.5999533  ...  0.          0.14860755\n","   1.692225  ]\n"," [ 5.3709157   0.46356567  0.91485681 ...  1.48415679  2.11658345\n","   0.70396175]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["\n","NMF_model2 = NMF(alpha_H= 0.001, \n","                #alpha_W= 0.001, \n","                l1_ratio=0.4, \n","                n_components=10\n","                ,init = 'random'\n","                )\n","nmf_output2 = NMF_model2.fit_transform(data_vectorized)\n","print(nmf_output2)  # Model attributes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IIORqH8TV-Mj","executionInfo":{"status":"ok","timestamp":1678215123550,"user_tz":-480,"elapsed":27660,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"dc121fde-2abf-43e9-c95c-4c9bc3569da0"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.          0.          0.         ...  5.81002297 15.75114063\n","   0.        ]\n"," [12.73631359  3.7283255   1.50104599 ...  3.02307932  5.73646778\n","   5.69806346]\n"," [21.46166368  1.37763077  0.         ...  5.24802081  0.74717588\n","   4.52971796]\n"," ...\n"," [20.55231362  0.09204465  2.1172661  ...  4.43590157  0.51129029\n","   0.        ]\n"," [21.9155353   2.06141005  0.75427319 ...  0.          0.23256519\n","   2.65845015]\n"," [ 8.34499944  0.58903731  1.19882685 ...  1.86153893  3.04656771\n","   1.15779609]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#print(data_vectorized[:20][0:])\n","# Print the top words for each topic\n","feature_names = vectorizer.get_feature_names_out()\n","#for topic_idx, topic in enumerate(NMF_model.components_):\n","#    top_words = [feature_names[i] for i in topic.argsort()[:-15 - 1:-1]]\n","#    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n","\n","for topic_idx, topic in enumerate(NMF_model2.components_):\n","    top_words = [feature_names[i] for i in topic.argsort()[:-15 - 1:-1]]\n","    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggOelhZFfiqc","executionInfo":{"status":"ok","timestamp":1678215082640,"user_tz":-480,"elapsed":342,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"9a428f92-3a56-408a-c9df-57fbef6a16d2"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic 0: time, year, day, friend, think, school, tell, work, start, know, didst, thing, try, month, say\n","Topic 1: want, die, end, stop, pain, hurt, scar, sleep, wish, wake, badly, tire, dead, happy, death\n","Topic 2: feel, like, feeling, bad, know, make, thought, felt, way, right, think, sad, time, day, happy\n","Topic 3: help, need, know, try, post, advice, ask, tell, thought, therapy, seek, sure, therapist, thank, wont\n","Topic 4: na, wan, gon, die, shit, end, soon, tonight, probably, ill, cut, tomorrow, bad, id, hang\n","Topic 5: live, life, end, point, year, reason, worth, pain, lose, job, try, continue, happy, ill, family\n","Topic 6: people, hate, care, know, love, wish, world, good, life, person, think, hurt, thing, nt, shit\n","Topic 7: kill, think, tonight, reason, ill, try, self, plan, right, way, today, wish, dead, scar, soon\n","Topic 8: anymore, tire, know, hurt, tired, pain, care, handle, try, point, goodbye, deal, tonight, shit, alive\n","Topic 9: talk, need, friend, tell, say, lonely, right, want, message, chat, pm, listen, girl, person, somebody\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","# Save a model to disk\n","joblib.dump(NMF_model, 'sg_nmf.jl')\n","joblib.dump(nmf_output, 'sg_nmf_output.jl')"],"metadata":{"id":"ONfxsrNJMH_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyldavis pyLDAvis==3.4.0\n","import pyLDAvis\n","import pyLDAvis.lda_model\n","pyLDAvis.enable_notebook()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2z2jI-wb7-L","executionInfo":{"status":"ok","timestamp":1678210757684,"user_tz":-480,"elapsed":5952,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"7bff8a52-25ec-4399-e625-8abf068356d7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyldavis in /usr/local/lib/python3.8/dist-packages (3.4.0)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.2.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.2.0)\n","Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.5.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (3.1.2)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from pyldavis) (2.8.4)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.18)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.10.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (from pyldavis) (3.6.0)\n","Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.22.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyldavis) (57.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyldavis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyldavis) (2022.7.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.1.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim->pyldavis) (6.3.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim->pyldavis) (1.15.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->pyldavis) (2.1.2)\n"]}]},{"cell_type":"code","source":["print('The pyLDAvis-learn version is {}.'.format(pyLDAvis.__version__))\n","panel = pyLDAvis.lda_model.prepare(NMF_model, data_vectorized, vectorizer, mds='tsne')\n","panel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"xXtyl8uAeQeC","executionInfo":{"status":"error","timestamp":1678210760547,"user_tz":-480,"elapsed":4,"user":{"displayName":"Allan Chong","userId":"15510319759249360908"}},"outputId":"b0a54a19-aa0c-46e3-f00d-65802f3b50dd"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["The pyLDAvis-learn version is 3.4.0.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/pyLDAvis/lda_model.py:26: RuntimeWarning: invalid value encountered in true_divide\n","  return dists / dists.sum(axis=1)[:, None]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-c1f7e6e817b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The pyLDAvis-learn version is {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpanel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNMF_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tsne'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpanel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyLDAvis/lda_model.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyLDAvis/lda_model.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(lda_model, dtm, vectorizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# column dimensions of document-term matrix and topic-term distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# must match first before transforming to document-topic distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdoc_topic_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_doc_topic_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     return {'vocab': vocab,\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'doc_lengths'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyLDAvis/lda_model.py\u001b[0m in \u001b[0;36m_get_doc_topic_dists\u001b[0;34m(lda_model, dtm)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_doc_topic_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_row_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massume_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, W, H, update_H)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;31m# initialize or check W and H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_w_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;31m# scale the regularization terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36m_check_w_h\u001b[0;34m(self, X, W, H, update_H)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 )\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0m_check_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NMF (input H)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36m_check_init\u001b[0;34m(A, shape, whom)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Array passed to %s is full of zeros.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Array passed to NMF (input H) is full of zeros."]}]}]}